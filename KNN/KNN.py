# -*- coding: utf-8 -*-
"""KNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KNTtkkS2QuCQxyQ8a5GzGpuy40_dEu9d
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("/content/diabetes.csv")
data.head()

"""# **method 1 for calculating k**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import math
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
X_train,X_test,y_train,y_test=train_test_split(data.iloc[:,:7],data.iloc[:,-1],test_size=0.20,random_state=42)
# scaler=StandardScaler()
# X_train=scaler.fit_transform(X_train)
# X_test=scaler.transform(X_test)
knn=KNeighborsClassifier(n_neighbors=int(math.sqrt(X_train.shape[0])+1))
knn.fit(X_train,y_train)
y_pred=knn.predict(X_test)
accuracy_score(y_test,y_pred)

"""#**Method 2 for finding the K value using the Trail and error**"""

# max_accuracy=0
# k=1
# for i in range(1,X_train.shape[0],2):
#   knn=KNeighborsClassifier(n_neighbors=i)
#   knn.fit(X_train,y_train)
#   y_pred=knn.predict(X_test)
#   if(accuracy_score(y_test,y_pred)>max_accuracy):
#     max_accuracy=accuracy_score(y_test,y_pred)
#     k=i
knn=KNeighborsClassifier(n_neighbors=31)#k=31
knn.fit(X_train,y_train)
y_pred=knn.predict(X_test)
print(accuracy_score(y_test,y_pred))

"""##**Applying the KNN classifier on iris dataset**"""

iris_data=sns.load_dataset("iris")
iris_data.head()

X=iris_data.iloc[:,:4]
y=iris_data.iloc[:,-1]
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)
knn=KNeighborsClassifier(n_neighbors=int(np.sqrt(X_train.shape[0])))
knn.fit(X_train,y_train)
y_pred=knn.predict(X_test)
accuracy_score(y_test,y_pred)
def predict(knn,sepalLength,sepalWidth,petalLength,petalWidth):
  return knn.predict(np.array([[sepalLength,sepalWidth,petalLength,petalWidth]]))
sepalLength=float(input("Enter the sepal length: "))
sepalWidth=float(input("Enter the sepal width: "))
petalLength=float(input("Enter the petal length: "))
petalWidth=float(input("Enter the petal width: "))
print("Species: ",predict(knn,sepalLength,sepalWidth,petalLength,petalWidth))

a=np.linspace(-10,11,1000)
b=np.linspace(-20,21,1000)
XX,YY=np.meshgrid(a,b)
Z=XX**2+YY**2
plt.figure(figsize=(8, 6))
contour = plt.contour(XX, YY, Z, levels=20, cmap='viridis')
plt.clabel(contour, inline=True, fontsize=8)  # Add labels to contours
plt.title('2D Contour Plot of $Z = X^2 + Y^2$')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

"""##**Decision boundary for the KNN classifier**

### **trainine the model:**
"""

from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000,
                           n_features=2,
                           n_informative=1,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           n_classes=2,
                           random_state=41,class_sep=0.8)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=29)
knn.fit(X_train, y_train)
accuracy_score(y_test,knn.predict(X_test))

"""##**Creating the mesh Grid:**"""

x=np.arange(X_train[:,0].min(),X_train[:,0].max(),0.01)
y=np.arange(X_train[:,1].min(),X_train[:,1].max(),0.01)
XX,YY=np.meshgrid(x,y)

"""## **Classifying the points:**"""

data=np.array([XX.ravel(),YY.ravel()]).T
print(data.shape)
#predicting the labels for all the points in the data
labels=knn.predict(data)
labels

labels.shape

"""## **plotting the decision boundary:**"""

plt.figure(figsize=(8, 6))
# Plot the decision boundary
contour = plt.contourf(XX, YY, labels.reshape(XX.shape), levels=20, cmap='viridis', alpha=0.6)
plt.colorbar(contour)
# Plot the data points
plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color="green", edgecolor="k", label="Class 0 (Train)")
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color="red", edgecolor="k", label="Class 1 (Train)")
# plt.scatter(X_test[:, 0], X_test[:, 1], color="blue", edgecolor="k", label="Test Points", alpha=0.6)
# Adding titles and labels
plt.title('KNN Decision Boundary with Data Points')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()

plt.show()

"""##**Building KNN Classifier from Scratch:**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

#loading the dataset and splitting the data into train and test data
data=pd.read_csv("/content/diabetes.csv")
X=data.iloc[:,:7]
y=data.iloc[:,-1]
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)
data.head()

import numpy as np
from collections import Counter
class KNNClassifier:
    def __init__(self, k):
        self.k = k
    def fit(self, X_train, y_train):
        self.X_train = X_train.reset_index(drop=True)
        self.y_train = y_train.reset_index(drop=True)
    def predict(self, X_test):
        y_pred = []
        for i in range(X_test.shape[0]):
            distances = np.sqrt(np.sum((self.X_train - X_test.iloc[i]) ** 2, axis=1))
            k_nearest_indices = np.argsort(distances)[:self.k]
            y_pred.append(self.classify(k_nearest_indices))
        return np.array(y_pred)
    def classify(self, k_nearest_indices):
        k_nearest_labels = self.y_train.iloc[k_nearest_indices]
        return Counter(k_nearest_labels).most_common(1)[0][0]

knn=KNNClassifier(k=3)
knn.fit(X_train,y_train)
knn.predict(X_test)
accuracy_score(y_test,knn.predict(X_test))

"""##**Effect of K on The Model:**"""

from sklearn.datasets import make_classification
X, y = make_classification(n_samples=1000,
                           n_features=2,
                           n_informative=1,
                           n_redundant=0,
                           n_clusters_per_class=1,
                           n_classes=2,
                           random_state=41,class_sep=0.8)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier

def plot_decision_boundary(X, y, k):
    # Convert inputs to NumPy arrays if they're pandas objects
    X = np.array(X)
    Y = np.array(y)
    # Check dimensions of X and y
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x = np.arange(x_min, x_max, 0.1)
    y = np.arange(y_min, y_max, 0.1)
    XX, YY = np.meshgrid(x, y)
    # Flatten the grid to pass through the classifier
    grid_points = np.c_[XX.ravel(), YY.ravel()]
    # Train KNN
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X, Y)
    # Predict labels for the grid
    labels = knn.predict(grid_points)
    # Plot the decision boundary
    plt.figure(figsize=(8, 6))
    plt.contourf(XX, YY, labels.reshape(XX.shape), levels=20, cmap="viridis", alpha=0.6)
    plt.colorbar(label="Predicted Label")
    # Plot the data points
    plt.scatter(X[Y == 0,0], X[Y == 0,1], color="green", edgecolor="k", label="Class 0 (Train)",alpha=0.7)
    plt.scatter(X[Y == 1,0], X[Y == 1,1], color="red", edgecolor="k", label="Class 1 (Train)",alpha=0.7)
    # Add titles and labels
    plt.title(f"KNN Decision Boundary (k={k})")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.legend()
    plt.show()

"""##**For K=1:**"""

#decision boundary for k value 1
plot_decision_boundary(X_train,y_train,1)

#decision boundary for k value 4
plot_decision_boundary(X_train,y_train,4)

#decision boundary for k value sqrt(n)
plot_decision_boundary(X_train,y_train,int(np.sqrt(X_train.shape[0]))+1)

#decision boundary for k value n.. In this case the model will predict in test data as the class having the highest data points
plot_decision_boundary(X_train,y_train,X_train.shape[0])

