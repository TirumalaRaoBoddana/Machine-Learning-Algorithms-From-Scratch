# -*- coding: utf-8 -*-
"""Decision Trees.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13toZb2ResDw2_lAehX3GPz_-pYRhhUaN
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
warnings.filterwarnings("ignore", message="matplotlib.font_manager:findfont: Font family 'Arial' not found.")

iris=load_iris()
X=iris.data
y=iris.target

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#applying the algorithms
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier(criterion="gini")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#plotting the tree
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)
plt.show()

#social networking ads
dataset=pd.read_csv("/content/Social_Network_Ads.csv")
dataset.Gender.replace({"Male":1,"Female":0},inplace=True)
dataset.head()
X=dataset.iloc[:,1:-1].values
y=dataset.iloc[:,-1].values
print(X.shape)
print(y.shape)

#splitting the data into train and test data
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

#applying the algorithm
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier(criterion="gini",max_depth=3)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#printing the tree
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

#plotting the decision boundary
def plot_decision_boundary(X,y,max_depth):
  #mesh grid
  x_min,x_max=X[:,0].min()-1,X[:,0].max()+1
  y_min,y_max=X[:,1].min()-1,X[:,1].max()+1
  xx,yy=np.meshgrid(np.arange(x_min,x_max,0.1),np.arange(y_min,y_max,100))
  #creating the model
  model=DecisionTreeClassifier(criterion="gini",max_depth=max_depth)
  model.fit(X,y)
  #test data
  test_data=np.c_[xx.ravel(),yy.ravel()]
  print(test_data.shape)
  predicted=model.predict(test_data)
  contour = plt.contourf(xx, yy, predicted.reshape(xx.shape), levels=20, cmap='viridis', alpha=0.6)
  plt.colorbar(contour)
  plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k')
X=dataset.iloc[:,2:-1].values
y=dataset.iloc[:,-1].values
plot_decision_boundary(X,y,max_depth=None)

#for max depth 2
plot_decision_boundary(X,y,max_depth=1)

"""**Hyper parameters in Decision Tree Classifier:**

1. **criterion:**The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain
"""

from sklearn.tree import DecisionTreeClassifier
#using entropy
dtc=DecisionTreeClassifier(criterion="entropy")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#using the gini impurity
dtc=DecisionTreeClassifier(criterion="gini")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#using the log loss
dtc=DecisionTreeClassifier(criterion="log_loss")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

"""2.**Splitter**:The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose the best random split.
"""

dtc=DecisionTreeClassifier(splitter="best")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#it will select the random split
dtc=DecisionTreeClassifier(splitter="random")
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

"""

3.**max_depth**:The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.High value for the max depth parameter will lead to overfitting and lower value for the parameter will lead to the underfitting..

"""

dtc=DecisionTreeClassifier(max_depth=None)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

dtc=DecisionTreeClassifier(max_depth=1)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

dtc=DecisionTreeClassifier(max_depth=4)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

"""

4.**min_samples_split**:The minimum number of samples required to split an internal node

"""

dtc=DecisionTreeClassifier(min_samples_split=2)#default value(case of overfitting)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

dtc=DecisionTreeClassifier(min_samples_split=100)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

"""

5.**min_samples_leaf**:The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches.

"""

dtc=DecisionTreeClassifier(min_samples_leaf=2)#default value 2(case of overfitting)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

dtc=DecisionTreeClassifier(min_samples_leaf=40)#default value(case of overfitting) all the leaf nodes should have atleast 40 samples
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

"""

1. **max_features**:The number of features to consider when looking for the best split


*   If “sqrt”, then max_features=sqrt(n_features).
*   If “log2”, then max_features=log2(n_features).
*   If None, then max_features=n_features.

"""

dtc=DecisionTreeClassifier(max_features="log2")#default value(case of overfitting)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

"""

7.**min_impurity_decrease**:A node will be split if this split induces a decrease of the impurity greater than or equal to this value.

"""

dtc=DecisionTreeClassifier(min_impurity_decrease=0.1)#default value(case of overfitting)
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))
from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtc,filled=True)

"""**Hyper parameter Tuning Using GridSearchCv**"""

from sklearn.model_selection import GridSearchCV
params={"criterion":["gini","entropy","log_loss"],
        "splitter":["best","random"],
        "max_depth":[1,2,3,4,None]}
grid=GridSearchCV(DecisionTreeClassifier(),params,cv=10)
grid.fit(X_train,y_train)
print(grid.best_params_)
print(grid.best_score_)

"""**Grid Search Cv on diabetes dataset**"""

#importing the dataset
diabetes=pd.read_csv("/content/pima_indians_diabetis.csv")
X=diabetes.iloc[:,:-1].values
y=diabetes.iloc[:,-1].values

#splitting the data into train and test data
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

#importing the decision tree classifier
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier()
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print(accuracy_score(y_test,y_pred))

#perfoming the grid search cv on the data
from sklearn.model_selection import GridSearchCV
params={"criterion":["gini","entropy","log_loss"],
        "splitter":["best","random"],
        "max_depth":[1,2,3,4,5,6,7,8,9,10,None],
        }
grid=GridSearchCV(DecisionTreeClassifier(),params,cv=10)
grid.fit(X_train,y_train)
print(grid.best_params_)
print(grid.best_score_)

"""### **Regression Trees:**

#### A regression tree is a machine learning model that predicts a continuous target value by recursively splitting the dataset into smaller groups based on feature thresholds. At each terminal node (leaf), the prediction is typically the mean of the target variable for that group.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# Set a random seed for reproducibility
np.random.seed(42)
# Generate the feature (X)
X = np.linspace(0, 10, 100)  # 100 evenly spaced values between 0 and 10
# Generate the target (y) with a non-linear relationship and added noise
y = 3 * np.sin(X) + 0.5 * X + np.random.normal(0, 0.5, size=X.shape)
# Convert to a pandas DataFrame for better usability
df = pd.DataFrame({'Feature': X, 'Target': y})
df.to_csv('regression_data.csv', index=False)
# Display the first few rows of the dataset
print(df.head())
# Visualize the dataset
plt.scatter(X, y, color='blue', label='Data Points')
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Synthetic Dataset for Regression')
plt.legend()
plt.show()

#train test split
X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42,test_size=0.20)

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import accuracy_score
dtr=DecisionTreeRegressor(max_depth=3)
dtr.fit(X_train.reshape(-1,1),y_train)
y_pred=dtr.predict(X_test.reshape(-1,1))
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f"Mean Squared Error (MSE): {mse}")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R^2): {r2}")

from sklearn.tree import plot_tree
plt.figure(figsize=(10,10))
plot_tree(dtr,filled=True)

"""### **DtreeViz library to visualize the Decision Trees:**"""

# Commented out IPython magic to ensure Python compatibility.
import sys
import os
# %config InlineBackend.figure_format = 'retina' # Make visualizations look good
#%config InlineBackend.figure_format = 'svg'
# %matplotlib inline

if 'google.colab' in sys.modules:
  !pip install -q dtreeviz

import dtreeviz

#importing the dataset
dataset=pd.read_csv("/content/PlayTennis.csv")
X,y=dataset.iloc[:,:-1],dataset.iloc[:,-1]
features=dataset.columns[:-1]
target=dataset.columns[-1]
class_names=list(dataset.iloc[:,-1].unique())

#converting the categorical values to numerical values
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
X=X.apply(le.fit_transform)
y.replace({"Yes":1,"No":0},inplace=True)

#splitting the data into train and test
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

#training the model on the data
from sklearn.tree import DecisionTreeClassifier
dtc=DecisionTreeClassifier()
dtc.fit(X_train,y_train)

viz_model = dtreeviz.model(dtc,
                           X_train, y_train,
                           feature_names=features,
                           target_name=target,
                           class_names=["Yes","No"],
                           )

viz_model.view(scale=2,fontname="sans-serif")

viz_model.view(scale=2,fancy=False,fontname="sans-serif")

viz_model.view(scale=2,orientation='LR',fontname="sans-serif")

viz_model.view(x=X.iloc[0,:],scale=2,fontname="sans-serif")

