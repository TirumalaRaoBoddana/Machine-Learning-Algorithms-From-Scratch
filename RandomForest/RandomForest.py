# -*- coding: utf-8 -*-
"""Random Forest.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/169y3TfPMNGKliSnhmUsxZhZPstXsWc57
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import make_classification

X, y = make_classification(
    n_samples=1000,
    n_features=5,
    n_informative=3,
    n_redundant=1,
    n_classes=2,
    random_state=42
)
dataframe=pd.DataFrame(X,columns=['f1','f2','f3','f4','f5'])
dataframe['target']=y
dataframe.head()

def row_sampling(df,percentage):
  return df.sample(int(df.shape[0]*percentage))
def col_sampling(df,percentage):
  features=df.iloc[0:,0:-1].sample(int(df.shape[1]*percentage),axis=1)
  return pd.concat([features,df['target']],axis=1)
def combine_sampling(df,row_percentage,col_percentage):
  df1=row_sampling(df,row_percentage)
  df2=col_sampling(df1,col_percentage)
  return df2

"""### **Using Row Sampling:**"""

df1=row_sampling(dataframe,0.2)
df2=row_sampling(dataframe,0.2)
df3=row_sampling(dataframe,0.2)

from sklearn.tree import DecisionTreeClassifier
dtc1=DecisionTreeClassifier()
dtc2=DecisionTreeClassifier()
dtc3=DecisionTreeClassifier()
dtc1.fit(df1.iloc[0:,0:-1],df1['target'])
dtc2.fit(df2.iloc[0:,0:-1],df2['target'])
dtc3.fit(df3.iloc[0:,0:-1],df3['target'])

import warnings
warnings.filterwarnings('ignore')
p1=dtc1.predict(np.array([-0.038769,	-0.649239,	-0.224746,	-1.346275,	0.126879]).reshape(1,5))

p2=dtc2.predict(np.array([-0.038769,	-0.649239,	-0.224746,	-1.346275,	0.126879]).reshape(1,5))

p3=dtc3.predict(np.array([-0.038769,	-0.649239,	-0.224746,	-1.346275,	0.126879]).reshape(1,5))

max(p1,p2,p3)

"""### **Using Column sampling:**"""

df1=col_sampling(dataframe,0.8)
df2=col_sampling(dataframe,0.8)
df3=col_sampling(dataframe,0.8)
df1.dropna(inplace=True)
df2.dropna(inplace=True)
df3.dropna(inplace=True)
df1

df2

df3

from sklearn.tree import DecisionTreeClassifier
dtc1=DecisionTreeClassifier()
dtc2=DecisionTreeClassifier()
dtc3=DecisionTreeClassifier()
dtc1.fit(df1.iloc[0:,0:-1],df1['target'])
dtc2.fit(df2.iloc[0:,0:-1],df2['target'])
dtc3.fit(df3.iloc[0:,0:-1],df3['target'])

import warnings
warnings.filterwarnings('ignore')
p1=dtc1.predict(np.array([-0.649239,	-0.224746,	-0.038769,	-1.346275	]).reshape(1,4))

p2=dtc2.predict(np.array([-1.346275,	-0.224746,	0.126879,	-0.649239]).reshape(1,4))

p3=dtc3.predict(np.array([-0.649239,	-0.224746,	-1.346275,	0.126879]).reshape(1,4))

max(p1,p2,p3)

"""### **Using the combination:**"""

df1=combine_sampling(dataframe,0.2,0.8)
df2=combine_sampling(dataframe,0.2,0.8)
df3=combine_sampling(dataframe,0.2,0.8)
df1.dropna(inplace=True)
df2.dropna(inplace=True)
df3.dropna(inplace=True)
df1

df2

df3

from sklearn.tree import DecisionTreeClassifier
dtc1=DecisionTreeClassifier()
dtc2=DecisionTreeClassifier()
dtc3=DecisionTreeClassifier()
dtc1.fit(df1.iloc[0:,0:-1],df1['target'])
dtc2.fit(df2.iloc[0:,0:-1],df2['target'])
dtc3.fit(df3.iloc[0:,0:-1],df3['target'])

dataframe

import warnings
warnings.filterwarnings('ignore')
p1=dtc1.predict(np.array([-0.038769,-1.346275,-0.224746,0.126879]).reshape(1,4))

p2=dtc1.predict(np.array([-0.038769,-0.649239,0.126879,-1.346275]).reshape(1,4))

p3=dtc1.predict(np.array([-1.346275,-0.649239,-0.038769,-0.224746	]).reshape(1,4))

max(p1,p2,p3)

from sklearn.datasets import make_circles
import pandas as pd
import matplotlib.pyplot as plt

# Generate a circular classification dataset
X, y = make_circles(
    n_samples=1000,      # Number of samples
    noise=0.4,          # Noise level to add randomness
    factor=0.5,         # Scale factor between inner and outer circle
    random_state=42     # Random state for reproducibility
)

# Convert to DataFrame
df = pd.DataFrame(X, columns=["Feature1", "Feature2"])
df["Class"] = y

# Display the first few rows of the dataset
print(df.head())

# Visualize the dataset
plt.scatter(df["Feature1"], df["Feature2"], c=df["Class"], cmap="coolwarm", edgecolor="k")
plt.title("Circles Dataset")
plt.xlabel("Feature1")
plt.ylabel("Feature2")
plt.show()

"""### **Decision boundary for decision tree classifier**"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
dtc=DecisionTreeClassifier()
X_train,X_test,y_train,y_test=train_test_split(df.iloc[0:,0:-1],df['Class'],test_size=0.2)
dtc.fit(X_train,y_train)

predicted=dtc.predict(X_test)
predicted

accuracy_score(predicted,y_test)

def plot_decision_boundary(X, y,labels):
    # Convert inputs to NumPy arrays if they're pandas objects
    X = np.array(X)
    Y = np.array(y)
    # Check dimensions of X and y
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x = np.arange(x_min, x_max, 0.1)
    y = np.arange(y_min, y_max, 0.1)
    XX, YY = np.meshgrid(x, y)
    # Flatten the grid to pass through the classifier
    grid_points = np.c_[XX.ravel(), YY.ravel()]
    plt.figure(figsize=(8, 6))
    dtc=DecisionTreeClassifier()
    dtc.fit(X,Y)
    labels=dtc.predict(grid_points)
    # Plot the decision boundary
    plt.contourf(XX, YY, labels.reshape(XX.shape), levels=20, cmap="viridis", alpha=0.6)
    plt.colorbar(label="Predicted Label")
    # Plot the data points
    plt.scatter(X[Y == 0,0], X[Y == 0,1], color="green", edgecolor="k", label="Class 0 (Train)",alpha=0.7)
    plt.scatter(X[Y == 1,0], X[Y == 1,1], color="red", edgecolor="k", label="Class 1 (Train)",alpha=0.7)
    # Add titles and labels
    plt.title(f"decision tree decision boundary")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.legend()
    plt.show()
plot_decision_boundary(X_train,y_train,predicted)

"""###**Decision boundary for random forest**"""

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
rfc.fit(X_train,y_train)
predicted=rfc.predict(X_test)
accuracy_score(predicted,y_test)

def plot_decision_boundary(X, y,labels):
    # Convert inputs to NumPy arrays if they're pandas objects
    X = np.array(X)
    Y = np.array(y)
    # Check dimensions of X and y
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    x = np.arange(x_min, x_max, 0.1)
    y = np.arange(y_min, y_max, 0.1)
    XX, YY = np.meshgrid(x, y)
    # Flatten the grid to pass through the classifier
    grid_points = np.c_[XX.ravel(), YY.ravel()]
    plt.figure(figsize=(8, 6))
    rfc=RandomForestClassifier()
    rfc.fit(X,Y)
    labels=rfc.predict(grid_points)
    # Plot the decision boundary
    plt.contourf(XX, YY, labels.reshape(XX.shape), levels=20, cmap="viridis", alpha=0.6)
    plt.colorbar(label="Predicted Label")
    # Plot the data points
    plt.scatter(X[Y == 0,0], X[Y == 0,1], color="green", edgecolor="k", label="Class 0 (Train)",alpha=0.7)
    plt.scatter(X[Y == 1,0], X[Y == 1,1], color="red", edgecolor="k", label="Class 1 (Train)",alpha=0.7)
    # Add titles and labels
    plt.title(f"decision tree decision boundary")
    plt.xlabel("Feature 1")
    plt.ylabel("Feature 2")
    plt.legend()
    plt.show()
plot_decision_boundary(X_train,y_train,predicted)

"""###**Bagging vs Random Forest**"""

from sklearn.ensemble import BaggingClassifier
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
X, y = make_classification(
    n_redundant=0,
    n_features=5,
    n_informative=5,
    n_clusters_per_class=1,
)
df = pd.DataFrame(X, columns=["Feature1", "Feature2","Feature3","Feature4","Feature5"])
df["Class"] = y
bagging = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    max_features=2,
    random_state=42,
)
X_train,X_test,y_train,y_test=train_test_split(df.iloc[0:,0:-1],df['Class'],test_size=0.2)
bagging.fit(X_train, y_train)
predicted = bagging.predict(X_test)
accuracy = accuracy_score(y_test, predicted)
print(f"Accuracy: {accuracy:.2f}")
plt.figure(figsize=(12, 8))
plot_tree(bagging.estimators_[2], filled=True)
plt.show()

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(
    max_features=2,
    bootstrap=True,
)
rfc.fit(X_train,y_train)
predicted=rfc.predict(X_test)
accuracy_score(predicted,y_test)
print(f"Accuracy: {accuracy:.2f}")
plt.figure(figsize=(12, 8))
plot_tree(bagging.estimators_[0], filled=True)
plt.show()

"""###**Hyper parameter tuning in random forest**"""

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

#loading the dataset
dataset=pd.read_csv("/content/placementdata.csv")
dataset

#convert the categorical data into numerical data
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
dataset['ExtracurricularActivities']=le.fit_transform(dataset['ExtracurricularActivities'])
dataset['PlacementTraining']=le.fit_transform(dataset['PlacementTraining'])
dataset["PlacementStatus"]=le.fit_transform(dataset["PlacementStatus"])

#training different models and calculating the accuracy score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
#splitting the data into train and test data
X_train,X_test,y_train,y_test=train_test_split(dataset.iloc[0:,0:-1],dataset['PlacementStatus'],test_size=0.2)
lr=LogisticRegression()
dtc=DecisionTreeClassifier()
rfc=RandomForestClassifier()
svc=SVC()
knn=KNeighborsClassifier()

#fitting the models
lr.fit(X_train,y_train)

dtc.fit(X_train,y_train)

rfc.fit(X_train,y_train)

svc.fit(X_train,y_train)

knn.fit(X_train,y_train)

lr_predicted=lr.predict(X_test)
dtc_predicted=dtc.predict(X_test)
rfc_predicted=rfc.predict(X_test)
svc_predicted=svc.predict(X_test)
knn_predicted=knn.predict(X_test)

lr_accuracy=accuracy_score(lr_predicted,y_test)
dtc_accuracy=accuracy_score(dtc_predicted,y_test)
rfc_accuracy=accuracy_score(rfc_predicted,y_test)
svc_accuracy=accuracy_score(svc_predicted,y_test)
knn_accuracy=accuracy_score(knn_predicted,y_test)
print("accuracy score for logistic regression: ",lr_accuracy)
print("accuracy score for decision tree classifier: ",dtc_accuracy)
print("accuracy score for random forest classifier: ",rfc_accuracy)
print("accuracy score for support vector classifier: ",svc_accuracy)
print("accuracy score for k nearest neighbors classifier: ",knn_accuracy)

from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(oob_score=True,bootstrap=True)
rfc.fit(X_train,y_train)
predicted=rfc.predict(X_test)
print("accuracy score: ",accuracy_score(predicted,y_test))
print("oob score: ",rfc.oob_score_)

"""###**Grid Search CV**"""

from sklearn.model_selection import GridSearchCV
param_grid={
    'n_estimators':[10,50,100,200,None],
    'max_depth':[None,5,10,20,30],
    'min_samples_split':[2,5,10,20],
    'min_samples_leaf':[10,20],
}

rfc=RandomForestClassifier()
grid=GridSearchCV(estimator=rfc,param_grid=param_grid,cv=5)
grid.fit(X_train,y_train)

grid.best_params_

#training the random forest classifier using above parameters
rfc=RandomForestClassifier(n_estimators=50,min_samples_split=5,min_samples_leaf=10,max_depth=30)
rfc.fit(X_train,y_train)

predicted=rfc.predict(X_train)
print("accuracy score ",accuracy_score(predicted,y_train))

"""###**Out of Bag Samples**

In a Random Forest model, each decision tree is trained on a bootstrap sample, which is created by randomly selecting data points with replacement from the original dataset. Some data points are not selected during this sampling processâ€”these are known as out-of-bag (OOB) samples.

###**Feature Importance**
"""

dataframe=pd.read_csv("/content/sample_data/mnist_train_small.csv")
dataframe.head()

#training the random forest classifier on the above data
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
rfc.fit(dataframe.iloc[0:,1:],dataframe['6'])

print(rfc.feature_importances_.shape)
feature_importances=rfc.feature_importances_

sns.heatmap(feature_importances.reshape(28,28))

"""**Calculation:**"""

from sklearn.datasets import make_classification
X,y=make_classification(
    n_samples=10,
    n_classes=2,
    n_features=2,
    n_informative=2,
    n_redundant=0,
    random_state=42,

)
X.shape

y

#training the model
from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier()
rfc.fit(X,y)

from sklearn.tree import plot_tree
plt.figure(figsize=(8,8))
plot_tree(rfc.estimators_[0],filled=True)
plt.show()

#printing the feature importances
print(rfc.feature_importances_)

"""###**Calculating feature importances for the Random Forest algorithm**"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
#training the random forest algorithm
rfc=RandomForestClassifier()
X_train,X_test,y_train,y_test=train_test_split(dataframe.iloc[0:,0:-1],dataframe['6'],test_size=0.2)
rfc.fit(X_train,y_train)

#predicting the values
predicted=rfc.predict(X_test)
print("accuracy score: ",accuracy_score(predicted,y_test))

#prediction
import random
random_index=random.randint(0,X_test.shape[0])
random_sample=X_test.iloc[random_index]
#this sample we need to give as input to the model to predict
import matplotlib.pyplot as plt
plt.imshow(random_sample.values.reshape(28,28),cmap="gray")

print(rfc.predict(np.array([random_sample])))

from sklearn.tree import plot_tree
feature_importances=[]
for i in range(rfc.n_estimators):
  feature_importances.append(rfc.feature_importances_)
feature_importances=np.array(feature_importances)

feature_importances.mean(axis=0)

rfc.feature_importances_

