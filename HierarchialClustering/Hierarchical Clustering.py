# -*- coding: utf-8 -*-
"""Hierarchical Clustering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ERXvTT5o_TDRHmGE3UUVS5kf5YfpbH0u
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("/content/hierarchical-clustering.csv")

df.head()

#shape of the dataset
df.shape

#describe
df.describe()

#null values
df.isnull().sum()

#info
df.info()

#feature extraction
X=df[['Annual Income (k$)','Spending Score (1-100)']]

#standardizing the data
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
X_scaled=scaler.fit_transform(X)

X_scaled

#plotting the data in 2d plane
plt.scatter(X_scaled[:,0],X_scaled[:,1])
plt.xlabel("X")
plt.ylabel("Y")
plt.show()

#plottting the dendrogram
from scipy.cluster.hierarchy import dendrogram, linkage
Z= linkage(X_scaled,method="ward")
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

#apply the k means clustering on this data and find the suelette score
from sklearn.cluster import KMeans
km=KMeans(n_clusters=5)
kmlabels=km.fit_predict(X)

X["kmlabel"]=kmlabels

X

plt.figure(figsize=(8, 6))

plt.scatter(X.iloc[:, 0][X["kmlabel"] == 0], X.iloc[:, 1][X["kmlabel"] == 0],
            color="green", label="Cluster 0")

plt.scatter(X.iloc[:, 0][X["kmlabel"] == 1], X.iloc[:, 1][X["kmlabel"] == 1],
            color="red", label="Cluster 1")

plt.scatter(X.iloc[:, 0][X["kmlabel"] == 2], X.iloc[:, 1][X["kmlabel"] == 2],
            color="blue", label="Cluster 2")

plt.scatter(X.iloc[:, 0][X["kmlabel"] == 3], X.iloc[:, 1][X["kmlabel"] == 3],
            color="yellow", label="Cluster 3")

plt.scatter(X.iloc[:, 0][X["kmlabel"] == 4], X.iloc[:, 1][X["kmlabel"] == 4],
            color="orange", label="Cluster 4")

plt.xlabel("Annual Income (k$)")
plt.ylabel("Spending Score (1-100)")
plt.title("Clustering using K Means Clustering algorithm")
plt.legend()
plt.show()

#calculating the silhouette_score for kmeans clustering
from sklearn.metrics import silhouette_score
print("silhouette_score for k means clustering algorithm: ",silhouette_score(X,X["kmlabel"]))

from sklearn.cluster import AgglomerativeClustering
agc=AgglomerativeClustering(n_clusters=5,linkage="ward")
aglabels=agc.fit_predict(X)

X["aglabel"]=aglabels

X

plt.figure(figsize=(8, 6))

plt.scatter(X.iloc[:, 0][X["aglabel"] == 0], X.iloc[:, 1][X["aglabel"] == 0],
            color="green", label="Cluster 0")

plt.scatter(X.iloc[:, 0][X["aglabel"] == 1], X.iloc[:, 1][X["aglabel"] == 1],
            color="red", label="Cluster 1")

plt.scatter(X.iloc[:, 0][X["aglabel"] == 2], X.iloc[:, 1][X["aglabel"] == 2],
            color="blue", label="Cluster 2")

plt.scatter(X.iloc[:, 0][X["aglabel"] == 3], X.iloc[:, 1][X["aglabel"] == 3],
            color="yellow", label="Cluster 3")

plt.scatter(X.iloc[:, 0][X["aglabel"] == 4], X.iloc[:, 1][X["aglabel"] == 4],
            color="orange", label="Cluster 4")

plt.xlabel("Annual Income (k$)")
plt.ylabel("Spending Score (1-100)")
plt.legend()
plt.title("Clustering using aglomerate algorithm")
plt.show()

#calculating the silhouette_score for kmeans clustering
from sklearn.metrics import silhouette_score
print("silhouette_score for k means clustering algorithm: ",silhouette_score(X,X["aglabel"]))

class CustomAgglomerativeClustering:
  def __init__(self, n_clusters=2, linkage="ward"):
      valid = ["single", "complete", "average", "ward"]
      if linkage not in valid:
          raise ValueError("Linkage must be one of: single, complete, average, ward")

      self.n_clusters = n_clusters
      self.linkage = linkage

  def euclidean_distance(self,point1,point2):

    return np.sqrt(np.sum((point1 - point2) ** 2))

  def compute_cluster_distance(self, cluster_a, cluster_b, dist_matrix, X):

    """Computes distance between clusters based on linkage"""

    if self.linkage == "single":
        return min(dist_matrix[a][b] for a in cluster_a for b in cluster_b)

    elif self.linkage == "complete":
        return max(dist_matrix[a][b] for a in cluster_a for b in cluster_b)

    elif self.linkage == "average":
        distances = [dist_matrix[a][b] for a in cluster_a for b in cluster_b]
        return sum(distances) / len(distances)

    elif self.linkage == "ward":
        # sizes
        nA = len(cluster_a)
        nB = len(cluster_b)

        # centroids
        meanA = np.mean(X[cluster_a], axis=0)
        meanB = np.mean(X[cluster_b], axis=0)

        # squared euclidean distance between centroids
        centroid_dist_sq = np.sum((meanA - meanB)**2)

        # Ward distance formula
        return (nA * nB) / (nA + nB) * centroid_dist_sq

  def fit_predict(self, X_df):

    X = X_df.values
    n_samples = X.shape[0]

    # Step 1: each point its own cluster
    clusters = [[i] for i in range(n_samples)]

    # Step 2: pairwise distance matrix
    dist_matrix = np.zeros((n_samples, n_samples))
    for i in range(n_samples):
        for j in range(i+1, n_samples):
            d = self.euclidean_distance(X[i], X[j])
            dist_matrix[i][j] = dist_matrix[j][i] = d

    # Step 3: keep merging until desired number of clusters
    while len(clusters) > self.n_clusters:

        min_dist = float("inf")
        p1 = p2 = -1

        for i in range(len(clusters)):
            for j in range(i+1, len(clusters)):
                d = self.compute_cluster_distance(clusters[i], clusters[j], dist_matrix, X)
                if d < min_dist:
                    min_dist = d
                    p1, p2 = i, j

        # merge clusters p2 into p1
        clusters[p1] += clusters[p2]
        clusters.pop(p2)
    labels = np.zeros(X.shape[0], dtype=int)
    for i in range(len(clusters)):
        for idx in clusters[i]:
            labels[idx] = i

    return labels

cag=CustomAgglomerativeClustering(n_clusters=5)
clusters=cag.fit_predict(X)

#calculate the silhouette_score custom agglomerative algorithm
from sklearn.metrics import silhouette_score
print("silhouette_score for k means clustering algorithm: ",silhouette_score(X,clusters))

X["label"]=clusters

X

plt.figure(figsize=(8, 6))

plt.scatter(X.iloc[:, 0][X["label"] == 0], X.iloc[:, 1][X["label"] == 0],
            color="green", label="Cluster 0")

plt.scatter(X.iloc[:, 0][X["label"] == 1], X.iloc[:, 1][X["label"] == 1],
            color="red", label="Cluster 1")

plt.scatter(X.iloc[:, 0][X["label"] == 2], X.iloc[:, 1][X["label"] == 2],
            color="blue", label="Cluster 2")

plt.scatter(X.iloc[:, 0][X["label"] == 3], X.iloc[:, 1][X["label"] == 3],
            color="yellow", label="Cluster 3")

plt.scatter(X.iloc[:, 0][X["label"] == 4], X.iloc[:, 1][X["label"] == 4],
            color="orange", label="Cluster 4")

plt.xlabel("Annual Income (k$)")
plt.ylabel("Spending Score (1-100)")
plt.legend()
plt.title("Clustering using custom aglomerate algorithm")
plt.show()

"""#**Working with the 3d data**"""

import numpy as np
from sklearn.datasets import make_blobs
import plotly.express as px
import pandas as pd

# 1. Generate clean 3D cluster data
X, y = make_blobs(
    n_samples=300,
    centers=4,
    cluster_std=1.0,
    n_features=3,
    random_state=42
)

# 2. Generate random noise points
noise_points = np.random.uniform(
    low=-15, high=15, size=(40, 3)   # 40 noise/outlier points
)

# 3. Combine the original data with noise
X_noisy = np.vstack([X, noise_points])

# 4. Create labels (original clusters + noise = -1)
labels = np.hstack([y, np.full(len(noise_points), -1)])

# 5. Create DataFrame for Plotly
df = pd.DataFrame(X_noisy, columns=["Feature 1", "Feature 2", "Feature 3"])

# # Plot using Plotly
fig = px.scatter_3d(
    df,
    x="Feature 1",
    y="Feature 2",
    z="Feature 3",
    # color="blue",
    opacity=0.8,
    # symbol="label",
    title="3D Dataset for Agglomerative Clustering (Plotly)"
)

fig.update_traces(marker=dict(size=5))
fig.show()

df

#plottting the dendrogram
from scipy.cluster.hierarchy import dendrogram, linkage
Z= linkage(df,method="ward")
plt.figure(figsize=(10, 5))
dendrogram(Z)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

#apply the k means clustering algorithm
from sklearn.cluster import KMeans
km=KMeans(n_clusters=3,max_iter=100)
labels=km.fit_predict(df)
#calculate the silhouette_score
from sklearn.metrics import silhouette_score
print("silhouette_score for k means clustering algorithm: ",silhouette_score(df,labels))

len(labels)

df["kmlabel"]=labels

#apply agglomerative clustering
from sklearn.cluster import AgglomerativeClustering
agg=AgglomerativeClustering(n_clusters=3,linkage="ward")
labels=agg.fit_predict(df)
#calculate the silhouette_score
from sklearn.metrics import silhouette_score
print("silhouette_score for k means clustering algorithm: ",silhouette_score(df,labels))

df["aglabel"]=labels

#plot the data with labels

# # Plot using Plotly
fig = px.scatter_3d(
    df,
    x="Feature 1",
    y="Feature 2",
    z="Feature 3",
    color="kmlabel",
    opacity=0.8,
    symbol="kmlabel",
    title="3D Dataset for Kmeans clustering Clustering (Plotly)"
)

fig.update_traces(marker=dict(size=5))
fig.show()

#plot the data with labels

# # Plot using Plotly
fig = px.scatter_3d(
    df,
    x="Feature 1",
    y="Feature 2",
    z="Feature 3",
    color="aglabel",
    opacity=0.8,
    symbol="aglabel",
    title="3D Dataset for Agglomerative Clustering (Plotly)"
)

fig.update_traces(marker=dict(size=5))
fig.show()

