# -*- coding: utf-8 -*-
"""Bagging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ve29BViagRGcJbt5m-msa-JV4nvt9jPz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""## **Bagging**

Bagging, or Bootstrap Aggregating, is an ensemble learning technique that improves the accuracy and stability of machine learning models by combining multiple weak learners. It reduces variance and prevents overfitting, making models more robust. Bagging is particularly useful for high-variance models like decision trees.
"""

#loading the dataset
df=sns.load_dataset('iris')

df.drop(columns={"sepal_length","petal_width"},inplace=True)

#label encoding the target column
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
df["species"]=le.fit_transform(df["species"])

#splitting the data into train and test data
from sklearn.model_selection import train_test_split
x=df.drop(columns={"species"})
y=df['species']
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

x

y.value_counts()

plt.scatter(x.iloc[:,0][y==0],x.iloc[:,1][y==0],color="green")
plt.scatter(x.iloc[:,0][y==1],x.iloc[:,1][y==1],color="red")
plt.scatter(x.iloc[:,0][y==2],x.iloc[:,1][y==2],color="blue")
plt.xlabel("sepal_wodth")
plt.ylabel("petal_length")
plt.show()

#creating samples with replacement
data=pd.concat([x_train,y_train],axis=1)
sample1=data.sample(frac=0.8,replace=True)
sample2=data.sample(frac=0.8,replace=True)
sample3=data.sample(frac=0.8,replace=True)

sample1

#training the models with the samples
from sklearn.tree import DecisionTreeClassifier
dtc1=DecisionTreeClassifier()
dtc2=DecisionTreeClassifier()
dtc3=DecisionTreeClassifier()
dtc1.fit(sample1.iloc[0:,0:2],sample1.iloc[0:,-1])
dtc2.fit(sample2.iloc[0:,0:2],sample2.iloc[0:,-1])
dtc3.fit(sample3.iloc[0:,0:2],sample3.iloc[0:,-1])

test_data=pd.concat([x_test,y_test],axis=1)
print(test_data)

def find_most_frequent(arr):
    counts = {}
    for item in arr:
        if item in counts:
            counts[item] += 1
        else:
            counts[item] = 1
    most_frequent = max(counts, key=counts.get)
    return most_frequent

test_data.index

#prediction on the test data
y_pred1=dtc1.predict([test_data.iloc[16,0:2]])
y_pred2=dtc2.predict([test_data.iloc[16,0:2]])
y_pred3=dtc3.predict([test_data.iloc[16,0:2]])
predicted_value=find_most_frequent(np.array([y_pred1,y_pred2,y_pred3]).flatten())
print(predicted_value)
print(test_data.species.iloc[16])

"""## **Pasting:**

Pasting is a variant of bagging where the key difference is sampling without replacement instead of with replacement. It still follows the same ensemble learning principles as bagging but changes how training subsets are selected.
"""

#creating samples without replacement
data=pd.concat([x_train,y_train],axis=1)
sample1=data.sample(frac=0.8)
sample2=data.sample(frac=0.8)
sample3=data.sample(frac=0.8)

#training the models with the samples
from sklearn.tree import DecisionTreeClassifier
dtc1=DecisionTreeClassifier()
dtc2=DecisionTreeClassifier()
dtc3=DecisionTreeClassifier()
dtc1.fit(sample1.iloc[0:,0:2],sample1.iloc[0:,-1])
dtc2.fit(sample2.iloc[0:,0:2],sample2.iloc[0:,-1])
dtc3.fit(sample3.iloc[0:,0:2],sample3.iloc[0:,-1])

test_data=pd.concat([x_test,y_test],axis=1)
print(test_data.species)

#prediction on the test data
y_pred1=dtc1.predict([test_data.iloc[0,0:2]])
y_pred2=dtc2.predict([test_data.iloc[0,0:2]])
y_pred3=dtc3.predict([test_data.iloc[0,0:2]])
predicted_value=find_most_frequent(np.array([y_pred1,y_pred2,y_pred3]).flatten())
print(predicted_value)
print(test_data.species.iloc[0])

"""##**Random Subspaces**

Random Subspaces is a variant of Bagging where randomness is introduced by selecting random subsets of features instead of (or in addition to) random subsets of data samples. This helps decorrelate base models and improve generalization.
"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
data = load_iris()
X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)
random_subspaces_model = BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=10,
    max_features=2,
    bootstrap=False,
    bootstrap_features=True,
    random_state=42
)
random_subspaces_model.fit(X_train, y_train)
y_pred = random_subspaces_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Random Subspaces Classifier Accuracy: {accuracy:.4f}')

"""##**Bagging Classifier**"""

from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.datasets import make_classification

#loading the dataset
X,y=make_classification(
    n_samples=1000,
    n_features=10,
    n_informative=10,
    n_redundant=0,
    random_state=42,
)
X.shape

#train test split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

#training with single deciosion tree
dtc=DecisionTreeClassifier()
dtc.fit(X_train,y_train)
predicted_values=dtc.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))

"""**1)Standard Bagging:**"""

#using bagging classifier
bagging_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    bootstrap=True,
    random_state=42
)
bagging_model.fit(X_train,y_train)
predicted_values=bagging_model.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))

#no.of rows used for each model
print(bagging_model.estimators_samples_[0].shape)

#no.of cols used for each model
print(bagging_model.estimators_features_[0].shape)

"""**2)Random Subspaces:**"""

#Random subspaces in bagging classifier
bagging_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    bootstrap_features=True,
    random_state=42,
    max_features=0.8,
)
bagging_model.fit(X_train,y_train)
predicted_values=bagging_model.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))

#no.of rows used for each model
print(bagging_model.estimators_samples_[0].shape)

#no.of cols used for each model
print(bagging_model.estimators_features_[0].shape)

"""**3)Pasting:**"""

#pasting in bagging classifier
bagging_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    random_state=42,
    bootstrap=False,
)
bagging_model.fit(X_train,y_train)
predicted_values=bagging_model.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))

#no.of rows used for each model
print(bagging_model.estimators_samples_[0].shape)

#no.of cols used for each model
print(bagging_model.estimators_features_[0].shape)

"""**4)Random Patches:**"""

#using random patches in bagging classifier
bagging_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    random_state=42,
    bootstrap=True,
    bootstrap_features=True,
    max_features=0.8,
    max_samples=0.8,
  )
bagging_model.fit(X_train,y_train)
predicted_values=bagging_model.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))

#no.of rows used for each model
print(bagging_model.estimators_samples_[0].shape)

#no.of cols used for each model
print(bagging_model.estimators_features_[0].shape)

"""**OOB Score in bagging:**"""

#using bagging classifier
bagging_model=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=300,
    bootstrap=True,
    random_state=42,
    oob_score=True,
)
bagging_model.fit(X_train,y_train)
predicted_values=bagging_model.predict(X_test)
print("accuracy score:",accuracy_score(y_test,predicted_values))
print("oob score: ",bagging_model.oob_score_)

"""#using Bagging classifier to classify the handdigits"""

#importing the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score,confusion_matrix
from sklearn.ensemble import BaggingClassifier

#importing the dataset
df=pd.read_csv("/content/sample_data/mnist_train_small.csv")

df.shape

df.rename(
    columns={"6":"label"},inplace=True
)

df["label"].value_counts().sort_index()

##distribution of the labels
import seaborn as sns
sns.barplot(df["label"].value_counts().sort_index())
plt.xlabel("Digits")
plt.ylabel("Frequency count of the images of each catrgory")
plt.title("frequency count of images of each digit")
plt.show()

#visualizing the digits
for i in range(10):
  plt.subplot(2,5,i+1)
  plt.imshow(df.iloc[i,1:].values.reshape(28,28),cmap="gray")
  plt.title(df.iloc[i,0])
  plt.axis("off")
plt.show()

"""#Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
X=df.drop(columns=["label"])
y=df["label"]

#train test split
X_train,X_test,y_train,y_test=train_test_split(
    X,y,test_size=0.2,random_state=42,
)

X_train.shape

y_train.shape

X_test.shape

y_test.shape

#applying the decision tree classifier
dtc=DecisionTreeClassifier()
dtc.fit(X_train,y_train)
y_pred=dtc.predict(X_test)
print("accuracy score of the model is: ",accuracy_score(y_test,y_pred))

#prediction
y_pred=dtc.predict([X_test.iloc[1]])
print(y_pred)

plt.imshow(df.iloc[1,1:].values.reshape(28,28),cmap="gray")
plt.title(dtc.predict(X_test.iloc[0:1]))

"""#prediction using the unseen data"""

test_data=pd.read_csv("/content/sample_data/mnist_test.csv")

test_data.rename(
    columns={"7":"label"},inplace=True
)

y_pred=dtc.predict(test_data.drop(columns=["label"]).values)
print("accuracy score of the model is: ",accuracy_score(test_data["label"],y_pred))

# visualizing the digits
indices = np.random.randint(0, test_data.shape[0], 10)

plt.figure(figsize=(10, 4))

for idx, i in enumerate(indices):
    plt.subplot(2, 5, idx + 1)
    plt.imshow(test_data.iloc[i, 1:].values.reshape(28, 28), cmap="gray")
    pred = dtc.predict([test_data.iloc[i, 1:].values])
    plt.title(f"Pred: {pred[0]}")
    plt.axis("off")

plt.show()

"""#using the baggingclassifier"""

#loading the train and test data
train_data=pd.read_csv("/content/sample_data/mnist_train_small.csv")
test_data=pd.read_csv("/content/sample_data/mnist_test.csv")

train_data.shape

test_data.shape

#changing the output column to label
train_data.rename(
    columns={
        "6":"label"
    },inplace=True,
)
test_data.rename(
    columns={
        "7":"label"
    },inplace=True,
)

# #splittin the train data
# X_train,X_test,y_train,y_test=train_test_split(
#     train_data.drop(columns=["label"]),
#     train_data["label"],
#     test_size=0.2,
#     random_state=42
# )

#fitting the bagging classifier model
bag=BaggingClassifier(
    estimator=DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=0.40,
    bootstrap=True,
    random_state=42,
)

bag.fit(train_data.iloc[:,1:],train_data["label"])

bag.estimators_samples_[0].shape

bag.estimators_features_[0].shape

test_data["label"].values

"""#Prediction using unseen data"""

y_pred=bag.predict(test_data.iloc[:,1:].values)
print("accuracy score of the model is: ",accuracy_score(test_data["label"].values,y_pred))

# visualizing the digits
indices = np.random.randint(0, test_data.shape[0], 10)

plt.figure(figsize=(10, 4))

for idx, i in enumerate(indices):
    plt.subplot(2, 5, idx + 1)
    plt.imshow(test_data.iloc[i, 1:].values.reshape(28, 28), cmap="gray")
    pred = dtc.predict([test_data.iloc[i, 1:].values])
    plt.title(f"Pred: {pred[0]}")
    plt.axis("off")
plt.show()

"""#**BaggingRegressor**"""

import numpy as np

# 2D nonlinear dataset
np.random.seed(42)

X_2d = np.linspace(-5, 5, 200).reshape(-1, 1)
y_2d = np.sin(X_2d) + 0.3 * np.random.randn(200, 1)

plt.scatter(X_2d,y_2d)

"""#DecisionTreeregressor"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
X_train,X_test,y_train,y_test=train_test_split(X_2d,y_2d,test_size=0.25,random_state=42)

from sklearn.model_selection import GridSearchCV
cv=GridSearchCV(
    estimator=DecisionTreeRegressor(),
    param_grid={
        "max_depth":[1,2,3,4,5,6,7,8,9,10,None],
        "criterion":["squared_error","friedman_mse"],
        "splitter":["best","random"],
    }
)
cv.fit(X_train,y_train)

cv.best_estimator_

cv.best_params_

cv.best_score_

from sklearn.tree import DecisionTreeRegressor
dtr=DecisionTreeRegressor(max_depth=8,splitter="random",criterion="squared_error")
dtr.fit(X_train,y_train)

y_pred=dtr.predict(X_test)
print("r2 score of the model is: ",r2_score(y_test,y_pred))

X_curve = np.linspace(X_train.min(), X_train.max(), 500).reshape(-1, 1)

# Predictions for the curve
y_curve = dtr.predict(X_curve)

# Plot
plt.figure(figsize=(10,6))

# Scatter the original training data
plt.scatter(X_train, y_train, color="blue", s=20, label="Training Data", alpha=0.6)

# Model curve
plt.plot(X_curve, y_curve, color="red", linewidth=2.5, label="Decision Tree Curve")

plt.title("Decision Tree Regression Curve")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.grid(True)
plt.show()

"""#Using the Bagging Regressor

#Applying the gridsearch cv on the bagging regressor
"""

from sklearn.ensemble import BaggingRegressor
from sklearn.model_selection import GridSearchCV
cv=GridSearchCV(
    estimator=BaggingRegressor(),
    param_grid={
        "n_estimators":[10,50,100,200],
        "max_samples":[0.25,0.5,0.75],
        "bootstrap":[True,False],
    }
)

cv.fit(X_train,y_train)

cv.best_estimator_

cv.best_params_

cv.best_score_

from sklearn.ensemble import BaggingRegressor
bgr=BaggingRegressor(
    estimator=DecisionTreeRegressor(max_depth=8,splitter="random",criterion="squared_error"),
    n_estimators=200,
    max_samples=0.25,
    bootstrap=True,

)

bgr.fit(X_train,y_train)

y_pred=bgr.predict(X_test)
print("r2 score of the model is: ",r2_score(y_test,y_pred))

X_curve = np.linspace(X_train.min(), X_train.max(), 500).reshape(-1, 1)

# Predictions for the curve
y_curve = bgr.predict(X_curve)

# Plot
plt.figure(figsize=(10,6))

# Scatter the original training data
plt.scatter(X_train, y_train, color="blue", s=20, label="Training Data", alpha=0.6)

# Model curve
plt.plot(X_curve, y_curve, color="red", linewidth=2.5, label="Decision Tree Curve")

plt.title("Decision Tree Regression Curve")
plt.xlabel("X")
plt.ylabel("y")
plt.legend()
plt.grid(True)
plt.show()

